---
title: "Regresión logística"
---

## Análisis de clasificación

La clasificación supervisada es una tarea muy frecuente en todas las áreas de análisis de datos. Existe un gran número de algoritmos desarrollados tanto por la estadística (regresión logística, análisis discriminante) como por la inteligencia artificial (redes neuronales, árboles de decisión, redes bayesianas) diseñados para realizar las tareas propias de la clasificación.

Vamos a centrarnos en el análisis de regresión logística, una técnica para el análisis de variables dependientes categóricas con dos categorías (**dicotómicas**) o más (**polinómicas**). Sirve para modelar la probabilidad de ocurrencia de un evento como función de otros factores, y responder preguntas como:

-   ¿Qué factores explican la victoria/derrota de un candidato en unas elecciones?

-   ¿Qué variables determinan que una persona vote?

-   ¿Qué factores incrementan/disminuyen el riesgo de caer en la pobreza?

-   ¿Cómo podemos explicar el abandono escolar?

-   etc

El análisis de regresión logística pertenece al grupo de *Modelos Lineales Generalizados* (GLM por sus siglas en inglés), y usa como función de enlace la función *logit*.

## Regresión logística vs regresión lineal

El modelo de regresión lineal no es válido cuando la variable respuesta no tiene una distribución normal. Por ejemplo: respuestas si/no, conteos, probabilidades, etc.

Al igual que la regresión lineal, la regresión logística busca:

-   predecir/explicar una VD a partir de una o mas VI,

-   medir el grado de relación de la VD con las VI

-   comprobar su significatividad

A diferencia de la regresión lineal:

-   los coeficientes de regresión se estiman por el procedimiento de **Máxima Verosimilitud**, que busca maximizar la probabilidad de ocurrencia del evento que se analiza

## Supuestos básicos

**Compartidos** con la Regresión Lineal:

-   Tamaño muestral elevado

-   Introducción de VIs relevantes

-   Variables predictoras continuas o dicotómicas

-   Ausencia de colinealidad entre las VIs

-   Aditividad

**Específicos**:

-   No-linealidad: La función de vinculación logit es no-lineal. Esto implica que **el cambio en la VD producido por el incremento de una unidad en la VI depende del valor que tome la variable**. Es menos importante en los extremos de las VI, y mas importante en los valores centrales.

![](images/clipboard-2977121164.png){fig-align="center" width="300"}

-   Heterocedasticidad: En regresión logística se asume heterocedasticidad (varianza de los residuos no constante). Es lo contrario que en la regresión lineal, ya que la representación de la regresión logística no es lineal, sino que se busca que debe existir varianza en los residuos no constante.

## La ecuación de la regresión logística

La función de enlace logit, utilizada principalmente en modelos de regresión logística, es como hemos mencionado no lineal, ya que transforma una combinación lineal de predictores en probabilidades mediante la fórmula:

$$
\ln\left(\frac{p(x)}{1 - p(x)}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p
$$

En esta ecuación, la VD aparece en una forma que no es directamente interpretable (concretamente, el logaritmo neperiano de la razón de probabilidades). Haciendo transformaciones, podemos expresar la probabilidad de ocurrencia del suceso de la siguiente manera:

$$
p(x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p)}} = \sigma\left(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p\right)
$$

donde:

$$
\sigma(x) = \frac{e^x}{1 + e^x} = \frac{1}{1 + e^{-x}}
$$

## Ejercicio de regresión logística

Vamos a hacer un sencillo ejercicio de regresión logística usando los datos del la encuesta preelectoral CIS 2023. En concreto, vamos a estimar la probabilidad de que un individuo *i* tenga intención de votar a un parido *p* en las elecciones generales de 2023. Dado que se trata de un ejercicio de clase, vamos a incluir unas pocas variables y no vamos a tomar en consideración los casos perdidos (indecisos, etc). En consecuencia, los resultados no sirven para predicción electoral, solo practicar.

Como de costumbre, abrimos fichero “Limpieza de datos” en su versión más reciente.

A partir de la variable intención de voto (´INTENCIONG\`), vamos a crear nuestra variable dependiente de intención voto VOX (**intovox**).

```{r message=FALSE, warning=FALSE, include=FALSE}
source("00_datos/source.r")
```

```{r}
# Intención de voto
table(datos$INTENCIONG)
# Intención voto VOX
datos <- datos %>%
  mutate(
    intvox = case_when(
      INTENCIONG == 3 ~ 1,            
      INTENCIONG >= 9977 ~ NA,  
      TRUE ~ 0))

table(datos$intvox, useNA = "ifany")
```

Como variables independientes, vamos a utilizar las variables “edad”, “hombre”, “estudios_universitarios”, “ecoesp”, “ideol” y “recuerdo19” que ya tenemos preparadas de clases anteriores. Una vez tenemos todas las variables preparadas, procedemos a crear el data.frame **data** con el conjunto de variables que vamos a incluir nuestros análisis y eliminamos los casos perdidos.

*Que existan tantos casos perdidos en la variable dependiente (5772) no importa en este caso al ser un ejemplo, pero a la hora de hacer un modelo de predicción real se deberían imputar todos estos valores para que el modelo sea más fiable*.

```{r}
datos_log <- datos %>%
  dplyr::select(intvox, hombre, estudios_universitarios, edad, ecoesp, ideol, recuerdo19) %>%
  drop_na()

summary(datos_log)
```

## Estimación del modelo

Ya podemos estimar la regresión logística. De las 4 variables dependientes que tenemos, vamos a empezamos por calcular la probabilidad de votar a VOX frente a otros partidos, en función de la ideología, recuerdo de voto en 2019, opinión sobre la economía en España y perfil socio-demográfico de la persona. Usamos la función `glm()` con *link function* binominal.

```{r}
library(MASS)
m.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19, data = datos_log, family = "binomial")
summary(m.vox)
```

## Significatividad de las variables

Al igual que en la regresión lineal, contrastamos las siguientes hipótesis:

-   H~0~:βi=0 -\> la VI~i~ no tiene efecto sobre la VD

-   H~a~:βi!=0 -\>la VI~i~ sí tiene efecto sobre la VD

Como ya sabemos, podemos rechazar la hipótesis nula siempre que:

-   p-valor\<0.05, NC:95%

-   p-valor\<0.01, NC:99%

-   p-valor\<0.001, NC:99.9%

Si además queremos calcular los intervalos de confianza, podemos usar la función `cofint()`

```{r}
confint(m.vox)
```

## Interpretación de los coeficientes

Los estimadores representan el logaritmo del cociente de probabilidades. Por ejemplo, ceteris paribus:

-   Ideología: Para cada punto que aumenta la ideología, el log de la probabilidad de votar a VOX (versus votar otro partido) aumenta en 0.290559.

-   Votó PP en 2019: el logaritmo de la probabilidad de votar a VOX en 2023 es 0.478263 mayor que entre los que en 2019 votaron al PP que entre los que votaron al PSOE (categoría de referencia).

Como se puede observar, esta interpretación de los coeficientes es muy poco intuitiva. Este tipo de coeficiente es útil si lo que nos interesa es conocer la dirección del efecto (signo positivo o negativo) y el nivel de significación (p-valor). Si por el contrario estamos interesados en interpretar el valor coeficiente, tenemos dos alternativas mejores: expresar los coeficientes como *odds ratio* o calcular las probabilidades de ocurrencia del evento.

## Odds ratio

Con el *odds* *ratio* lo que hago es exponenciar el coeficiente, osea e exponencial de Beta: $e^{\text{coef}}$ y es la frecuencia de ocurrencia de un suceso sobre la frecuencia de su no ocurrencia:

-   Odds ratio \>1: la variable tiene un efecto positivo sobre la probabilidad de ocurrencia del suceso.

-   0 \> Odds ratio \<1: la variable tiene un efecto negativo sobre la probabilidad de ocurrencia del suceso.

-   Odds ratio =1: la variable no tiene efecto sobre la probabilidad de ocurrencia del suceso.

En la tabla:

-   Odds ratio~ideología~=e^0.290559^=1.337. Para cada punto que nos movemos a la derecha en la escala de ideología, la probabilidad de votar a VOX (sobre votar a otro partido) aumenta en un factor de 1.337 (cuando el resto de variables permanecen constantes). O si se quiere expresar en porcentaje.

    -   (1,337 - 1)\* 100 = 0,337 \* 100 = 33,7%

    Sale 1,33, el cual es el mismo valor del coeficiente solo que ahora lo calcula las betas elevadas a $e$ para desahacer el logaritmo neperiano.

-   Odds ratio~votó PP(2019)~= e^0.478263^=1,61. La probabilidad de votar a VOX (sobre votar a otro partido) es 1,61 veces mayor entre las personas que en 2019 votaron al PP que entre las personas que en 2019 votaron al PSOE (cuando el resto de variables permanecen constantes). O si se quiere expresar en porcentaje.

    -   (1,61 - 1)\* 100 = 0,61 \* 100 = 61%

-   En el caso de que el coeficiente sea negativo, como por ejemplo “opinión sobre la situación de la economía en España” haríamos así: Odds ratio~ecoesp~=e^-1.352050^=0.259. En este caso, la probabilidad de votar a VOX es 0.259 veces menor entre las personas que consideran que la economía nacional va bien, que entre los que consideran que la economía nacional va mal (ceteris paribus). Expresado en porcentaje, sería:

    -   (1 - 0.259)\* 100 = 0.741 \* 100 = 74%

Para obtener los coeficientes exponenciados usamos la función `exp()`: Con este código le estoy pidiendo los odds ratio porque le estoy pidiendo el exponente de los coeficientes de la regresión logística.

```{r}
exp(coef(m.vox))
```

Para ponerlo todo en una tabla, usamos la función `cbind` (*column bind*), que nos permite unir la columna de los coeficientes y la de los intervalos de confianza.

```{r}
exp(cbind(OR = coef(m.vox), confint(m.vox)))
```

**Comparar coeficientes**

Los odds ratio se pueden comparar entre sí para saber qué variable es más explicativa o está asociada de manera más fuerte con la VD. Pero OJO! Para comparar un odds ratio mayor que uno (relación positiva) con un odds ratio menor que uno (relación negativa), es necesario calcular el valor inverso de uno de los datos porque el rango es distinto. Por ejemplo:

-   ecoesp: 1/0,259=3,86

Cuando hacemos esto para poder comparar debemos tener en cuenta que este coeficiente es para la variable de referencia. En este caso 3,86 es para la economíanegativa, porque estamos dandole la vuelta al numerador y denominador.

La inversión de variables también es útil en el caso de las variables dicotómicas para comprobar el supuesto contrario al establecido. Por ejemplo, la probabilidad de las mujeres (en lugar del de los hombres) es de 1/2,13=0,47.

Se puede hacer para cualquier tipo de variable, en el caso de la categóricas se invierte a su variable de referencias, y en el caso de las de escala (como la variable ideologia) lo entendemos como el aumento de una unidad.

## Probabilidades predichas

Como alternativa a los coeficientes y a los odds ratio se pueden calcular las probabilidades predichas. Las probabilidades predichas son la mejor manera de entender las variables de un modelo. Para calcularlas, primero debemos crear un *data.frame* con los valores que queremos que tomen las variables independientes en nuestras predicciones.

```{r}
data1 <- with(datos_log, data.frame(ideol = mean(ideol), recuerdo19 = c("PSOE","PP","VOX","Podemos","Ciudadanos", "Más Madrid", "Otros", "En blanco"), edad = mean(edad), hombre="Hombre", estudios_universitarios="con EU", ecoesp="negativa"))

head(data1, 8)
```

Este código predice con la media de ideología en la muestra en vez de con ideología = 5, para todos los recuerdos de voto, que esté en la media de edad de la muestra, que sea hombre, con estudios y con una opinión negativa para la economía.

Es importante que las variables en este *data.frame* tengan el mismo nombre que las variables en la regresión logística anterior. Una vez creado el *data.frame*, ya podemos pedirle a R que calcule las probabilidades predichas.

```{r}
data1$probpredichas_vox<- predict(m.vox, newdata = data1, type = "response")
data1[, c(2, 7)]  #le pido que muestre todas las filas de las columnas 2 (recuerdo voto) y 7(probabilidad predicha)
```

Los resultados muestran la probabilidad predicha de votar a VOX en 2023 para hombres con edad media, ideología media y con estudios universitarios, que opinan que la economía en España va mal, según el partido que habían votado en las elecciones anteriores.

## Efectos marginales

El paquete *margins* responde a un intento de trasladar el comando “margins” de Stata a R, como un método genérico para calcular los efectos marginales -o efectos parciales- de las variables independientes. Por ejemplo, vamos calcular el efecto marginal de las VIs en nuestro modelo

```{r}
library(margins)
margins_vox <- margins(m.vox)

# Resumen
summary_margins <- summary(margins_vox) 
summary_margins
```

También podemos representarlos gráficamente. El gráfico a continuación representa la columna AME (*Average Marginal Effect*) y las columnas *low* and *upper* (bandas de confianza inferior y superior).

```{r}
# Convertimos el resumen en un data.frame para poder hacer un gráfico
data_to_plot <- data.frame(
  factor = summary_margins$factor,
  AME = summary_margins$AME,
  lower = summary_margins$lower,
  upper = summary_margins$upper)
```

```{r}
ggplot(data_to_plot, aes(x = AME, y = factor)) +
  geom_point(color = "blue", size = 3) +  # Puntos para los AME
  geom_errorbarh(aes(xmin = lower, xmax = upper), height = 0.2, color = "black") +  # Barras de error
  geom_vline(xintercept = 0, linetype = "dashed", color = "grey") +  # Línea vertical en 0
  labs(
    title = "Average Marginal Effects (AME) with Confidence Intervals",
    x = "AME",
    y = "Variables") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),  # Centrar título
    axis.text.y = element_text(size = 10))  # Ajustar tamaño de texto

```

Interpretación: Cuando se dice que un efecto marginal es 0.3882, significa que haber votado a VOX en 2019 está asociado con un aumento promedio de 0.17 unidades en la variable dependiente. En el contexto de una probabilidad o un porcentaje, como suele ser el caso en la regresión logística o modelos similares, un efecto marginal de 0.3882 corresponde a un aumento del 38,82 puntos porcentuales.

Para hacer un modelo robusto puedes elegir poner las variables con sus valores que conformarían el escenario mas adverso. El escenario que más desfavorece la hipótesis planteada.

Para más información sobre opciones del paquete margins podéis consultar [aquí](https://www.rdocumentation.org/packages/margins/versions/0.3.23) y [aquí](https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html).

**Resumen**

La primera transformación es pasar el logaritmo al otro lado, es decir $e^{\text{coef}}$ –\> los odds ratios, porque al otro lado del igual queda $(\frac{p(x)}{1 - p(x)})$ , es decir la probabilidad del que el evento suceda partido de la probabilidad de que no suceda.

La última transfomación de la ecuación nos deja despejado $p(x)$, de esta manera podemos hacer probabilidades predichas de x sustituyendo los betas y las x (como hacías valores predichos en lm). Como la distribución de la muestra no es lineal los efectos de x no son iguales a lo largo de todo y, de esta manera podemos calcular casos específicos de y. Podemos conocer a lo largo de la regresión la probabilidad de y para cada x seleccionada por nosotros.

Por último podemos calcular los *Average* *Marginal* *Effects* (*margins* en Stata). El efecto marginal no es la probabilidad de ocurrencia de x, sino el fecto de la VI sobre la probabilidad de ocurrencia de x. Es la manera de conseguir el equivalente a los coeficientes de la regresión lineal porque calcular todas las y para ciertos valores de x y luego hace el promedio de esos impactos de la x. En el caso de lineales no hace falta transformar nada porque la media del efectos de una VI sobre todos los valores de y es igual a el efecto de de VI sobre un solo valor de y porque es un efecto estable a lo largo de toda la regresión.

AME se usa principalmente cuando tienes un interés particular por el efecto de una VI.

## Diagnóstico

#### Multicolinealidad

```{r}
library(rms)
logit.vif<- vif(m.vox)
logit.vif
```

Todos los valores están por debajo de 5. No parece que existan problemas de multicolinealidad

#### Heterocedasticidad

```{r}
library(lmtest)
logit.het<-bptest(m.vox)
logit.het
```

La hipótesis nula en este test es que la varianza de los residuos es constante. La evidencia permite rechazar la hipótesis nula, confirmando que se cumple el supuesto de heterocedasticidad (que es lo que buscamos).

## Presentación de los resultados

```{r}
library(stargazer)

stargazer(m.vox,
          type="text",
          dep.var.labels=c("Voto VOX"),
          covariate.labels=c("Hombre", "Estudios Universitarios", "Edad", "Valoración + economia", "Ideología", "Voto 2019: PP (cr:PSOE)", "Voto 2019: VOX (cr:PSOE)", "Voto 2019: Podemos (cr:PSOE)", "Voto 2019: Ciudadanos (cr:PSOE)", "Voto 2019: +Madrid (cr:PSOE)", "Voto 2019: Otros (cr:PSOE)", "Voto 2019: blanco (cr:PSOE)", "Constante"))
```

## Validación del modelos de clasificaicón

A continuación, vamos examinar cómo de bueno/malo es nuestro modelo a la hora de clasificar datos nuevos. Para ello, continuamos con el ejemplo anterior, en el que estimábamos la probabilidad de que un individiuo vote a un determinado partido. Se llama clasficación porque estoy intentado clasificar a los individuos en función de los valores de la VD, que en este caso es 0 y 1.

En primer lugar, creamos el conjunto de entrenamiento (60%) y test (40%). Como hay pocos 1 en la muestra (de la var dependiente), se amplia el % de casos destinados al test para evitar que dentro de este la proporción de 1 sea demasiado bajo como para comprobar el modelo.Se saca una muestra aleatoria y por eso ponermos el seed, para que nos escoja los mismos datos aleatorios y sea replicable.

```{r}
set.seed(123)
index <- 1:nrow(datos_log)
porc_test <- 0.40
# Dividir datos
test.data <- datos_log %>% sample_frac(porc_test)  
train.data <- datos_log %>% anti_join(test.data)
```

Creamos la variable **clase_real**, que corresponde a la variable dependiente (intentación voto a VOX) en el conjunto de test. Es la variable que luego comparemos con los valores estimados para ver nuestro nivel de acierto/error:

```{r}
clase_real <- test.data$intvox
```

Entrenamos el modelo (intención de voto a VOX) con los datos del train.data:

```{r}
library(MASS)
logit.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19, data = train.data, family = "binomial")
summary(logit.vox)
```

Después, calculamos los valores predichos en el conjunto de test. Estos son los valores que luego vamos a comparar con la “clase real”:

```{r}
predicted_logit<- predict(logit.vox, newdata=test.data, type="response")
head(predicted_logit)
```

Para valorar cómo de bien/mal clasifica nuestro modelo, vamos a calcular la curva ROC(*Receiver operating characteristics*) y el AUC (*Area under the curve*). Cuando hacemos una clasificación binaria, existen 4 tipos de resultados posibles:

-   True negative (TN): predecimos 0 y la clase real es 0

-   False negative (FN): predecimos 0 y la clase real es 1

-   True positive (TP): predecimos 1 y la clase real es 1

-   False positive (FP): predecimos 1 y la clase real es 0

A partir de estos resultados se construye la *matriz de confusión*:

![](images/clipboard-2798392842.png){fig-align="center" width="400"}

La matriz de confusión sive para calcular la **curva ROC**, que es la representación gráfico de la razón de Verdaderos Positivos (TPR) frente a la razón de falsos positivos (FPR):

-   Razón de verdaderos positivos (TPR - true positive rate): proporción de positivos reales que son correctamente identificados como positivos por el modelo. También se le conoce como sensibilidad o recall. Se calcula como:

$$
TPR = \frac{TP}{TP + FN}
$$

-   Razón de falsos positivos (FPR - false positive rate) = proporción de negativos reales que son incorrectamente identificados como positivos por el modelo. Se calcula como:

$$
FPR = \frac{FP}{FP + TN}
$$

Gráficamente, el espacio ROC se representa de la siguiente manera:

![](images/clipboard-2824448575.png){fig-align="center" width="400"}

Lo ideal es encontrar una curva que se acerque lo máximo posible al punto de clasificación perfecta. Si está en la línea, los resultados de la predicción son completamente aleatorios, y si está por debajo de esta el modelo predice aún peor que asignar cifras al azar.

Pintamos la curva ROC de nuestro modelo:

```{r}
library(ROCR)

# Curva ROC
pred_logit <-  prediction(predicted_logit, clase_real) # crea un objeto "predicción"
perf_logit <- performance(pred_logit, measure = "tpr", x.measure = "fpr") 
par(mfrow = c(1,1))
plot(perf_logit, lty=1, col="darkgrey", main = "Logit ROC Curve")
```

Esto se interpreta en función del area que hay debajo de la línea, lo ideal es que el area sea 1 (100%) es decir, que todo lo ha clasificado bien.

Calculamos el area bajo la curva (AUC). El AUC (Área Bajo la Curva) es una métrica que mide el desempeño de un modelo de clasificación binaria. Un valor de AUC:

-   1.0: El modelo clasifica perfectamente todas las instancias

-   0.5: El modelo no tiene poder predictivo (es como adivinar al azar)

-   \< 0.5: El modelo clasifica peor que al azar.

```{r}
auc.logit<- performance(pred_logit, measure = "auc", x.measure = "fpr") 
auc.logit@y.values 
```

En nuestro caso, AUC=0.920. Este valor indica que nuestro modelo clasifica bien en un 92% por de los casos.

Nota: AUC es un *S4 object system*, por eso las consultas de sus elementos son un poco diferentes a lo que hemos visto hasta ahora. Nosotros no vamos a entrar en esto, pero si teneís curiosidad podéis leer [este capítulo del libro de Hadley Wickham](http://adv-r.had.co.nz/S4.html).

## Comparación de modelos

Normalmente, la curva ROC se utiliza para comparar la precisión de diferentes algoritmos de clasificación (como regresión logística, Naive Bayes, Random Forest, LDA, etc). Aunque en nuestro caso solo hemos trabajado con regresión logística, vamos a estimar diferentes modelos a modo de ejemplo, pero sin profundizar en los detalles de su funcionamiento.

### Modelo de clasificación Random Forest

Empezamos con un [random forest](https://www.r-bloggers.com/2021/04/random-forest-in-r/)

```{r}
library(randomForest)

# Convertimos la variable intvoto en factor (de lo contrario, da error)
train.data$intvox <- as.factor(train.data$intvox)
test.data$intvox <- as.factor(test.data$intvox)

# Entrenamos el modelo Random Forest 
rf.vox <- randomForest(
  intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19,
  data = train.data,
  ntree = 500,      # número de árboles
  mtry = 2,         # número de predictores seleccionados aleatoriamente por árbol
  importance = TRUE # importancia de variables
)

# Importancia de las variables
importance(rf.vox) 
```

Le indicamos que haga 500 arboles; mtry es el número de predictores seecionados aleatoriamente. Importance indica que en los 500 arboles que variables han tenido más peso de manera sistemática en todos los modelos. Este valor no lo puedes interpretar pero te hace un ranking, cuanto más altos mejor (Accuracy y Gini). En vez de conocer la significatividad estadística tienemos la importancia de las variables en el modelo. El random forest va apartando poco a pode preditores que no sean relevantes para el modelo, según va haciendo los modelos de manera altearia.

Cuanto mayor es el número dentro del Accuracy, más relevante es esa variable dentro de la predicción. Que una variable sea más importante que otra no implica que sea mejor, ya que al no tener ninguna medida como el p-valor no se puede saber si la variable es significativa o no. Por tanto, que una variable sea más importante no la hace relevante en sí.

En este modelo el type se llama "prob" y luego si entre corchetes se pone \[1\] clasifica sobre los 0 y \[,2\] quiere decir que clasifica sobre los 1.

```{r}
# Calculamos los valores predichos en el conjunto de test
predicted_rf <- predict(rf.vox, newdata = test.data, type = "prob")[, 2]

# Creamos el objeto de predicción para ROC
pred_rf <- prediction(predicted_rf, clase_real) # valores predichos, valores reales

# Calculamos rendimiento (ROC)
perf_rf <- performance(pred_rf, measure = "tpr", x.measure = "fpr")

# Pintamos la curva ROC
par(mfrow = c(1,1)) 
plot(perf_rf, lty = 1, col = "gold", main = "Random Forest ROC Curve")
```

```{r}
# Calculamos el AUC
auc.rf<- performance(pred_rf, measure = "auc", x.measure = "fpr") 
auc.rf@y.values
```

En este caso, AUC es de 88,7%. Esto indica que en el 87% de los casos, el modelo clasifica correctamente.

### Modelo de clasificación Naive Bayes

Repetimos el proceso con el algoritmo [Naive Bayes](https://www.r-bloggers.com/2021/04/naive-bayes-classification-in-r/) (misma filosofía que un modelo bayesiano pero es otro modelo de clasificación).

```{r}
library(e1071)  

# Entrenamos el modelo Naive Bayes 
nb_model <- naiveBayes(
  intvox ~ hombre + estudios_universitarios + edad + ecoesp + ideol + recuerdo19, data = train.data)

# Calculamos los valores predichos en el conjunto de test
predicted_nb <- predict(nb_model, newdata = test.data, type = "raw")[, 2] # raw en esta librería equivale a prob en la de random.forest

# Creamos el objeto de predicción para ROC
pred_nb <- prediction(predicted_nb, clase_real) #comparar datos predichos con reales

# Calculamos el rendimiento (ROC)
perf_nb <- performance(pred_nb, measure = "tpr", x.measure = "fpr")

# Pintamos la curva ROC
par(mfrow = c(1,1)) # Configuración de un solo gráfico
plot(perf_nb, lty = 1, col = "steelblue", main = "Naive Bayes ROC Curve")

```

```{r}
# Calculamos el AUC
auc.nb<- performance(pred_nb, measure = "auc", x.measure = "fpr") 
auc.nb@y.values 
```

En este caso, el porcentaje de casos correctamente clasificados es 91,6%.

Para tener una visión global vamos a representar todas las curvas ROC de manera conjunta. Esto nos permite comparar los resultados de los tres algoritmos:

```{r}
par(mfrow = c(1,1))
plot(perf_logit, lty=1, col="darkgrey", main = "ROC Curves")
plot(perf_rf, lty=1, col="gold", add = TRUE)
plot(perf_nb, lty=1, col="steelblue", add = TRUE)
legend(0.4, 0.6,  
       c("Logit=0.920", "Random Forest=0.887", "Naive Bayes=0.916"), 
       lty = c(1,1,1),       
       bty = "n",
       col=c("darkgrey", "gold","steelblue"),
       cex = 0.7)
```

Es importante recordar que nuestros modelos sólo están teniendo en consideración a los individuos que han respondido a todas las preguntas incluidas en el modelo. De esta manera, estamos dejando fuera a muchos entrevistados que no ofrecen información sobre alguna de las variables, particularmente, ideología y recuerdo. Esto genera un importante sesgo. ¿Sería mejor dejar estas variables fuera? A modo de ejemplo, vamos a ver cuál hubiera sido el resultado de no haber incluido información sobre esas variables. Repitamos nuestro modelo original, esta vez excluyendo ideología y recuerdo de voto en el 2019.

```{r}
# Modelo sin ideología ni recuerdo de voto
logit2.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + ecoesp, data = train.data, family = "binomial")

# Predicción
predicted_logit2<- predict(logit2.vox, newdata=test.data, type="response")

# ROC
pred_logit2 <-  prediction(predicted_logit2, clase_real)
perf_logit2<- performance(pred_logit2, measure = "tpr", x.measure = "fpr") 

# AUC
auc.logit2 <- performance(pred_logit2, measure = "auc", x.measure = "fpr") 
auc.logit2@y.values
```

La AUC en este caso es considerablemente más baja. Comparamos ambos modelos gráficamente:

```{r}
par(mfrow = c(1,1))
plot(perf_logit, lty=1, col="darkgrey", main = "Logit ROC Curves")
plot(perf_logit2, lty=2, col="grey", add = TRUE)

legend(0.4, 0.6,  
       c("Logit=0.920", "Logit(sin ideología ni recuerdo de voto)=0.765"), 
       lty = c(1,2),       
       bty = "n",
       col=c("darkgrey", "grey"),
       cex = 0.7)
```

Como se puede observar en el gráfico arriba, la predicción es mucho peor cuando no disponemos de información sobre ideología y recuerdo de voto. Una alternativa sería imputar los valores perdidos en las variables que tienen un número elevado de NA antes de hacer la regresión.

## Mejora de modelos

Vamos a explorar qué ocurre si tomamos en consideración que algunas variables pueden no tener un efecto lineal. Por ejemplo, la edad. Sabemos por otros estudios que el perfil de edad de los votantes de Vox es diverso, pero el partido tiene un respaldo importante entre votantes de mediana edad (35-54 años). Para tomar en cuenta esto, vamos a elevar ambos la variable edad al cuadrado. Recordad que siempre hay que incluir el efecto principal y el cuadrático

```{r}
logit3.vox <- glm(intvox ~ hombre + estudios_universitarios + edad + I(edad^2) + ecoesp + ideol + recuerdo19,
  data = train.data, family = "binomial")
summary(logit3.vox)
```

En nuestro modelo, el efecto principal es positivo, y el cuadrático es negativo. Además, ambos son estadísticamente significativos. La combinación de estos efectos sugiere que la relación entre la variable independiente y la dependiente tiene forma de “U” invertida: La variable dependiente aumenta hasta cierto punto (el máximo) y luego comienza a disminuir.

En [este post](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-the-sign-of-the-quadratic-term-in-a-polynomial-regression/) se explica bastante bien la relación entre signos del efecto principal y cuadrático y la forma del efecto
